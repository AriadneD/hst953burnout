{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install transformers datasets accelerate peft\n",
    "#fsspec==2024.10.0\n",
    "!pip install sympy==1.13.3\n",
    "#pip install tiktoken\n",
    "#pip install triton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load model\n",
    "%%time\n",
    "#https://huggingface.co/microsoft/Phi-3.5-mini-instruct\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "#\"microsoft/Phi-3-small-8k-instruct\" #8B\n",
    "#\"microsoft/Phi-3.5-mini-instruct\" #\"microsoft/phi-2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Correct some special tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token value for padding\n",
    "tokenizer.eos_token = '<|eot_id|>'  # Set the end of sequence token\n",
    "\n",
    "#set cuda for GPU\n",
    "import torch\n",
    "torch.set_default_device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data \n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "with open('no_burnout.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# If the JSON data is a list of dictionaries, we can directly convert it to a DataFrame\n",
    "if isinstance(data, list):\n",
    "    df_no = pd.DataFrame(data)\n",
    "\n",
    "with open('clean_burnout.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# If the JSON data is a list of dictionaries, we can directly convert it to a DataFrame\n",
    "if isinstance(data, list):\n",
    "    df_yes = pd.DataFrame(data)\n",
    "\n",
    "df_no['label'] = 'nonburnedout'\n",
    "df_yes['label'] = 'burnedout' \n",
    "\n",
    "#combine and shuffle \n",
    "df_all = pd.concat([df_no, df_yes]).sample(frac=1, random_state=42, replace=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df_all['body']\n",
    "y = df_all['label']\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "smote = SMOTE(random_state=42)\n",
    "X, y = smote.fit_resample(X, y)\n",
    "print('Smote dataset shape %s' % Counter(y))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pipeline for model\n",
    "%%time\n",
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer = tokenizer,\n",
    "    max_new_tokens= 100,\n",
    "    #max_length = 1000,\n",
    "    truncation = True,\n",
    "    #padding = True,\n",
    "    #temperature = 0.1, #not needed b/c do_sample=False meaning it's deterministic\n",
    "    #top_k = 50,\n",
    "    #top_p = 0.95,\n",
    "    do_sample=False,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time:\", end_time - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "  # Regex pattern to find the last occurrence of \"output: \" and extract until the next '}'\n",
    "  pattern = r'\\'output\\': (.*?)\\n'#r'output: (.*?)\\}'\n",
    "\n",
    "  # Find all occurrences of the pattern\n",
    "  matches = re.findall(pattern, text)\n",
    "\n",
    "  # Get the last match (if any)\n",
    "  if matches:\n",
    "      last_match = str(matches[-1])\n",
    "      cleaned_text = re.sub(r'[^a-zA-Z]', '', last_match)\n",
    "      return cleaned_text\n",
    "  else:\n",
    "      return \"Not found\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "\n",
    "all_responses_original = []\n",
    "all_responses = []\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for index, post in tqdm(df_all.iterrows(), total=df_all.shape[0]):\n",
    "  prompt = f\"\"\"\n",
    "  The input is a post a person shared on the social media.\n",
    "  The output is a binary classification of this person being 'burned out' or 'non burned out'.\n",
    "  The task is provide the output based on the input.\n",
    "  Please provide the output with only the following based on the input text, and DO NOT iterate the input or give additional text beyond the output in your response:\n",
    "    - {{'output': 'burned out' or 'non burned out'}}\n",
    "\n",
    "  Here is the input: {post['title'] + \" \" + post['body']}\n",
    "  \"\"\"\n",
    "    #inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "  output = pipe(prompt)\n",
    "  response = clean_text(output[0]['generated_text'])\n",
    "  all_responses.append(response)\n",
    "  #not enough memory to save all these for long runs\n",
    "  all_responses_original.append(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the CSV file for writing\n",
    "df_x = df_all\n",
    "df_x['predictions'] = all_responses\n",
    "df_x['full generated text'] = all_responses_original\n",
    "df_x\n",
    "df_x.to_csv('pred_all_wTitle.csv', index=False)\n",
    "\n",
    "\"\"\" If running the two dataframes separately\n",
    "with open('pred_no_burnout_clean.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write each item from the list as a row\n",
    "    for item in all_responses:\n",
    "        writer.writerow([item])  # Write the item as a row (wrapped in a list)\n",
    "\n",
    "with open('pred_no_burnout_original.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write each item from the list as a row\n",
    "    for item in all_responses_original:\n",
    "        writer.writerow([item])  # Write the item as a row (wrapped in a list)\n",
    "\n",
    "\"\"\"\n",
    "print(\"output SAVED results to runtime, REMEMBER TO DOWNLOAD TO LOCAL MACHINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVED results to runtime, REMEMBER TO DOWNLOAD TO LOCAL MACHINE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
